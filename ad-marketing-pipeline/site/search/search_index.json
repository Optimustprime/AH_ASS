{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ad Marketing Pipeline","text":"<p>A real-time data pipeline for ad marketing budget management using Apache Spark, Delta Lake, and Azure Event Hubs on Databricks.</p>"},{"location":"#architecture-overview","title":"Architecture Overview","text":""},{"location":"#overview","title":"Overview","text":"<p>This pipeline processes ad click events in real-time to track advertiser spending against budgets and make serving decisions. It follows the medallion architecture pattern with Bronze, Silver, and Gold layers.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>For detailed API documentation, see:</p> <ul> <li>Processors</li> <li>Streaming</li> <li>Configuration</li> </ul>"},{"location":"#schema-diagram","title":"Schema Diagram","text":""},{"location":"api-reference/config/","title":"Configuration","text":""},{"location":"api-reference/config/#database-config","title":"Database Config","text":"<p>Unity Catalog configuration for the ad marketing pipeline.</p> Source code in <code>src\\config\\settings.py</code> <pre><code>@dataclass\nclass DatabaseConfig:\n    \"\"\"Unity Catalog configuration for the ad marketing pipeline.\"\"\"\n\n    catalog: str = \"ad_marketing_catalog\"\n    bronze_schema: str = \"bronze\"\n    silver_schema: str = \"silver\"\n    gold_schema: str = \"gold\"\n\n    @property\n    def bronze_table(self) -&gt; str:\n        return f\"{self.catalog}.{self.bronze_schema}.ad_click_events_raw\"\n\n    @property\n    def silver_table(self) -&gt; str:\n        return f\"{self.catalog}.{self.silver_schema}.ad_click_events_clean\"\n\n    @property\n    def gold_table(self) -&gt; str:\n        return f\"{self.catalog}.{self.gold_schema}.advertiser_spend\"\n</code></pre>"},{"location":"api-reference/config/#event-hub-config","title":"Event Hub Config","text":"<p>Event Hub configuration for streaming data.</p> Source code in <code>src\\config\\settings.py</code> <pre><code>@dataclass\nclass EventHubConfig:\n    \"\"\"Event Hub configuration for streaming data.\"\"\"\n\n    connection_string_key: str = \"EVENT_HUB_CONNECTION_STRING\"\n    scope: str = \"ahass-scope\"\n    event_hub_name: str = \"ad-clicks\"\n    checkpoint_location: str = \"/delta/checkpoints/ad_click_events_raw\"\n</code></pre>"},{"location":"api-reference/processors/","title":"Processors","text":""},{"location":"api-reference/processors/#gold-processor","title":"Gold Processor","text":"<p>Processes silver data to gold layer with business aggregations.</p> Source code in <code>src\\processors\\gold_processor.py</code> <pre><code>class GoldProcessor:\n    \"\"\"Processes silver data to gold layer with business aggregations.\"\"\"\n\n    def __init__(self, spark: SparkSession, db_config: DatabaseConfig):\n        \"\"\"\n        Initialize the gold processor.\n\n        Args:\n            spark: Spark session\n            db_config: Database configuration\n        \"\"\"\n        self.spark = spark\n        self.db_config = db_config\n        self.logger = logging.getLogger(__name__)\n\n    def read_silver_data(self, start_ts: datetime, end_ts: datetime) -&gt; DataFrame:\n        \"\"\"\n        Read silver data for a specific time window.\n\n        Args:\n            start_ts: Start timestamp\n            end_ts: End timestamp\n\n        Returns:\n            Filtered DataFrame from silver table\n        \"\"\"\n        silver_df = self.spark.read.table(self.db_config.silver_table)\n\n        filtered_df = silver_df.filter(\n            (col(\"timestamp\") &gt;= start_ts) &amp; (col(\"timestamp\") &lt; end_ts)\n        )\n\n        return filtered_df\n\n    def aggregate_spend_data(\n        self, df: DataFrame, start_ts: datetime, end_ts: datetime\n    ) -&gt; DataFrame:\n        \"\"\"\n        Aggregate spend data by advertiser.\n\n        Args:\n            df: Silver DataFrame\n            start_ts: Window start timestamp\n            end_ts: Window end timestamp\n\n        Returns:\n            Aggregated DataFrame with spend metrics\n        \"\"\"\n        gold_df = (\n            df.groupBy(\"advertiser\", \"advertiser_id\")\n            .agg(\n                sum(\"amount\").alias(\"gross_spend\"),\n                sum(when(col(\"is_valid\") == True, col(\"amount\")).otherwise(0)).alias(\n                    \"net_spend\"\n                ),\n                count(\"*\").alias(\"record_count\"),\n                spark_max(\"budget_value\").alias(\"budget_value\"),\n            )\n            .withColumn(\"window_start\", lit(start_ts))\n            .withColumn(\"window_end\", lit(end_ts))\n            .withColumn(\"can_serve\", col(\"net_spend\") &lt; col(\"budget_value\"))\n            .withColumn(\"spend_hour\", date_trunc(\"HOUR\", lit(start_ts)))\n            .withColumn(\"spend_day\", date_trunc(\"DAY\", lit(start_ts)))\n            .withColumn(\"spend_month\", date_trunc(\"MONTH\", lit(start_ts)))\n        )\n\n        return gold_df\n\n    def write_to_gold(self, df: DataFrame) -&gt; None:\n        \"\"\"\n        Write aggregated data to gold table.\n\n        Args:\n            df: Aggregated DataFrame to write\n        \"\"\"\n        df.write.format(\"delta\").mode(\"append\").partitionBy(\"spend_day\").saveAsTable(\n            self.db_config.gold_table\n        )\n\n        self.logger.info(f\"Data written to {self.db_config.gold_table}\")\n\n    def process_hour(\n        self, target_hour: Optional[datetime] = None, hours_back: int = 4\n    ) -&gt; None:\n        \"\"\"\n        Process a specific hour of data from silver to gold.\n\n        Args:\n            target_hour: Target hour to process (defaults to current hour)\n            hours_back: Number of hours back to process\n        \"\"\"\n        if target_hour is None:\n            target_hour = datetime.utcnow()\n\n        start_ts = target_hour.replace(minute=0, second=0, microsecond=0)\n        end_ts = start_ts + timedelta(hours=1)\n\n        self.logger.info(f\"Processing hour: {start_ts} to {end_ts}\")\n\n        silver_df = self.read_silver_data(start_ts, end_ts)\n        gold_df = self.aggregate_spend_data(silver_df, start_ts, end_ts)\n        self.write_to_gold(gold_df)\n</code></pre>"},{"location":"api-reference/processors/#src.processors.gold_processor.GoldProcessor.__init__","title":"<code>__init__(spark, db_config)</code>","text":"<p>Initialize the gold processor.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session</p> required <code>db_config</code> <code>DatabaseConfig</code> <p>Database configuration</p> required Source code in <code>src\\processors\\gold_processor.py</code> <pre><code>def __init__(self, spark: SparkSession, db_config: DatabaseConfig):\n    \"\"\"\n    Initialize the gold processor.\n\n    Args:\n        spark: Spark session\n        db_config: Database configuration\n    \"\"\"\n    self.spark = spark\n    self.db_config = db_config\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"api-reference/processors/#src.processors.gold_processor.GoldProcessor.read_silver_data","title":"<code>read_silver_data(start_ts, end_ts)</code>","text":"<p>Read silver data for a specific time window.</p> <p>Parameters:</p> Name Type Description Default <code>start_ts</code> <code>datetime</code> <p>Start timestamp</p> required <code>end_ts</code> <code>datetime</code> <p>End timestamp</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered DataFrame from silver table</p> Source code in <code>src\\processors\\gold_processor.py</code> <pre><code>def read_silver_data(self, start_ts: datetime, end_ts: datetime) -&gt; DataFrame:\n    \"\"\"\n    Read silver data for a specific time window.\n\n    Args:\n        start_ts: Start timestamp\n        end_ts: End timestamp\n\n    Returns:\n        Filtered DataFrame from silver table\n    \"\"\"\n    silver_df = self.spark.read.table(self.db_config.silver_table)\n\n    filtered_df = silver_df.filter(\n        (col(\"timestamp\") &gt;= start_ts) &amp; (col(\"timestamp\") &lt; end_ts)\n    )\n\n    return filtered_df\n</code></pre>"},{"location":"api-reference/processors/#src.processors.gold_processor.GoldProcessor.aggregate_spend_data","title":"<code>aggregate_spend_data(df, start_ts, end_ts)</code>","text":"<p>Aggregate spend data by advertiser.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Silver DataFrame</p> required <code>start_ts</code> <code>datetime</code> <p>Window start timestamp</p> required <code>end_ts</code> <code>datetime</code> <p>Window end timestamp</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Aggregated DataFrame with spend metrics</p> Source code in <code>src\\processors\\gold_processor.py</code> <pre><code>def aggregate_spend_data(\n    self, df: DataFrame, start_ts: datetime, end_ts: datetime\n) -&gt; DataFrame:\n    \"\"\"\n    Aggregate spend data by advertiser.\n\n    Args:\n        df: Silver DataFrame\n        start_ts: Window start timestamp\n        end_ts: Window end timestamp\n\n    Returns:\n        Aggregated DataFrame with spend metrics\n    \"\"\"\n    gold_df = (\n        df.groupBy(\"advertiser\", \"advertiser_id\")\n        .agg(\n            sum(\"amount\").alias(\"gross_spend\"),\n            sum(when(col(\"is_valid\") == True, col(\"amount\")).otherwise(0)).alias(\n                \"net_spend\"\n            ),\n            count(\"*\").alias(\"record_count\"),\n            spark_max(\"budget_value\").alias(\"budget_value\"),\n        )\n        .withColumn(\"window_start\", lit(start_ts))\n        .withColumn(\"window_end\", lit(end_ts))\n        .withColumn(\"can_serve\", col(\"net_spend\") &lt; col(\"budget_value\"))\n        .withColumn(\"spend_hour\", date_trunc(\"HOUR\", lit(start_ts)))\n        .withColumn(\"spend_day\", date_trunc(\"DAY\", lit(start_ts)))\n        .withColumn(\"spend_month\", date_trunc(\"MONTH\", lit(start_ts)))\n    )\n\n    return gold_df\n</code></pre>"},{"location":"api-reference/processors/#src.processors.gold_processor.GoldProcessor.write_to_gold","title":"<code>write_to_gold(df)</code>","text":"<p>Write aggregated data to gold table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Aggregated DataFrame to write</p> required Source code in <code>src\\processors\\gold_processor.py</code> <pre><code>def write_to_gold(self, df: DataFrame) -&gt; None:\n    \"\"\"\n    Write aggregated data to gold table.\n\n    Args:\n        df: Aggregated DataFrame to write\n    \"\"\"\n    df.write.format(\"delta\").mode(\"append\").partitionBy(\"spend_day\").saveAsTable(\n        self.db_config.gold_table\n    )\n\n    self.logger.info(f\"Data written to {self.db_config.gold_table}\")\n</code></pre>"},{"location":"api-reference/processors/#src.processors.gold_processor.GoldProcessor.process_hour","title":"<code>process_hour(target_hour=None, hours_back=4)</code>","text":"<p>Process a specific hour of data from silver to gold.</p> <p>Parameters:</p> Name Type Description Default <code>target_hour</code> <code>Optional[datetime]</code> <p>Target hour to process (defaults to current hour)</p> <code>None</code> <code>hours_back</code> <code>int</code> <p>Number of hours back to process</p> <code>4</code> Source code in <code>src\\processors\\gold_processor.py</code> <pre><code>def process_hour(\n    self, target_hour: Optional[datetime] = None, hours_back: int = 4\n) -&gt; None:\n    \"\"\"\n    Process a specific hour of data from silver to gold.\n\n    Args:\n        target_hour: Target hour to process (defaults to current hour)\n        hours_back: Number of hours back to process\n    \"\"\"\n    if target_hour is None:\n        target_hour = datetime.utcnow()\n\n    start_ts = target_hour.replace(minute=0, second=0, microsecond=0)\n    end_ts = start_ts + timedelta(hours=1)\n\n    self.logger.info(f\"Processing hour: {start_ts} to {end_ts}\")\n\n    silver_df = self.read_silver_data(start_ts, end_ts)\n    gold_df = self.aggregate_spend_data(silver_df, start_ts, end_ts)\n    self.write_to_gold(gold_df)\n</code></pre>"},{"location":"api-reference/processors/#silver-processor","title":"Silver Processor","text":"<p>Processes bronze data to silver layer with data cleaning and validation.</p> Source code in <code>src\\processors\\silver_processor.py</code> <pre><code>class SilverProcessor:\n    \"\"\"Processes bronze data to silver layer with data cleaning and validation.\"\"\"\n\n    def __init__(self, spark: SparkSession, db_config: DatabaseConfig):\n        \"\"\"\n        Initialize the silver processor.\n\n        Args:\n            spark: Spark session\n            db_config: Database configuration\n        \"\"\"\n        self.spark = spark\n        self.db_config = db_config\n        self.logger = logging.getLogger(__name__)\n\n    def read_bronze_data(self, start_ts: datetime, end_ts: datetime) -&gt; DataFrame:\n        \"\"\"\n        Read bronze data for a specific time window.\n\n        Args:\n            start_ts: Start timestamp\n            end_ts: End timestamp\n\n        Returns:\n            Filtered DataFrame from bronze table\n        \"\"\"\n        bronze_df = self.spark.read.table(self.db_config.bronze_table)\n\n        filtered_df = bronze_df.filter(\n            (col(\"timestamp\") &gt;= start_ts) &amp; (col(\"timestamp\") &lt; end_ts)\n        )\n\n        record_count = filtered_df.count()\n        self.logger.info(f\"Records fetched from Bronze: {record_count}\")\n\n        return filtered_df\n\n    def clean_and_transform(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Clean and transform bronze data for silver layer.\n\n        Args:\n            df: Raw bronze DataFrame\n\n        Returns:\n            Cleaned DataFrame ready for silver layer\n        \"\"\"\n        silver_df = (\n            df.filter(col(\"event_type\") == \"ad_click\")\n            .filter(col(\"amount\").isNotNull() &amp; (col(\"amount\") &gt; 0))\n            .filter(col(\"advertiser\").isNotNull())\n            .filter(col(\"advertiser_id\").isNotNull())\n            .withColumn(\"is_valid\", col(\"amount\") &gt; 0)\n            .withColumn(\"processed_at\", current_timestamp())\n            .withColumn(\"ingest_year\", year(col(\"timestamp\")))\n            .withColumn(\"ingest_month\", month(col(\"timestamp\")))\n            .withColumn(\"ingest_day\", dayofmonth(col(\"timestamp\")))\n            .withColumn(\"ingest_hour\", hour(col(\"timestamp\")))\n        )\n\n        return silver_df\n\n    def write_to_silver(self, df: DataFrame) -&gt; None:\n        \"\"\"\n        Write processed data to silver table.\n\n        Args:\n            df: Cleaned DataFrame to write\n        \"\"\"\n        df.write.format(\"delta\").mode(\"append\").partitionBy(\n            \"ingest_year\", \"ingest_month\", \"ingest_day\"\n        ).saveAsTable(self.db_config.silver_table)\n\n        self.logger.info(f\"Data written to {self.db_config.silver_table}\")\n\n    def process_hour(self, target_hour: Optional[datetime] = None) -&gt; None:\n        \"\"\"\n        Process a specific hour of data from bronze to silver.\n\n        Args:\n            target_hour: Target hour to process (defaults to current hour)\n        \"\"\"\n        if target_hour is None:\n            target_hour = datetime.utcnow()\n\n        start_ts = target_hour.replace(minute=0, second=0, microsecond=0)\n        end_ts = start_ts + timedelta(hours=1)\n\n        self.logger.info(f\"Processing hour: {start_ts} to {end_ts}\")\n\n        bronze_df = self.read_bronze_data(start_ts, end_ts)\n\n        if bronze_df.count() == 0:\n            self.logger.warning(\"No records found for the specified hour. Exiting.\")\n            return\n\n        silver_df = self.clean_and_transform(bronze_df)\n        self.write_to_silver(silver_df)\n</code></pre>"},{"location":"api-reference/processors/#src.processors.silver_processor.SilverProcessor.__init__","title":"<code>__init__(spark, db_config)</code>","text":"<p>Initialize the silver processor.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session</p> required <code>db_config</code> <code>DatabaseConfig</code> <p>Database configuration</p> required Source code in <code>src\\processors\\silver_processor.py</code> <pre><code>def __init__(self, spark: SparkSession, db_config: DatabaseConfig):\n    \"\"\"\n    Initialize the silver processor.\n\n    Args:\n        spark: Spark session\n        db_config: Database configuration\n    \"\"\"\n    self.spark = spark\n    self.db_config = db_config\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"api-reference/processors/#src.processors.silver_processor.SilverProcessor.read_bronze_data","title":"<code>read_bronze_data(start_ts, end_ts)</code>","text":"<p>Read bronze data for a specific time window.</p> <p>Parameters:</p> Name Type Description Default <code>start_ts</code> <code>datetime</code> <p>Start timestamp</p> required <code>end_ts</code> <code>datetime</code> <p>End timestamp</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered DataFrame from bronze table</p> Source code in <code>src\\processors\\silver_processor.py</code> <pre><code>def read_bronze_data(self, start_ts: datetime, end_ts: datetime) -&gt; DataFrame:\n    \"\"\"\n    Read bronze data for a specific time window.\n\n    Args:\n        start_ts: Start timestamp\n        end_ts: End timestamp\n\n    Returns:\n        Filtered DataFrame from bronze table\n    \"\"\"\n    bronze_df = self.spark.read.table(self.db_config.bronze_table)\n\n    filtered_df = bronze_df.filter(\n        (col(\"timestamp\") &gt;= start_ts) &amp; (col(\"timestamp\") &lt; end_ts)\n    )\n\n    record_count = filtered_df.count()\n    self.logger.info(f\"Records fetched from Bronze: {record_count}\")\n\n    return filtered_df\n</code></pre>"},{"location":"api-reference/processors/#src.processors.silver_processor.SilverProcessor.clean_and_transform","title":"<code>clean_and_transform(df)</code>","text":"<p>Clean and transform bronze data for silver layer.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Raw bronze DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Cleaned DataFrame ready for silver layer</p> Source code in <code>src\\processors\\silver_processor.py</code> <pre><code>def clean_and_transform(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Clean and transform bronze data for silver layer.\n\n    Args:\n        df: Raw bronze DataFrame\n\n    Returns:\n        Cleaned DataFrame ready for silver layer\n    \"\"\"\n    silver_df = (\n        df.filter(col(\"event_type\") == \"ad_click\")\n        .filter(col(\"amount\").isNotNull() &amp; (col(\"amount\") &gt; 0))\n        .filter(col(\"advertiser\").isNotNull())\n        .filter(col(\"advertiser_id\").isNotNull())\n        .withColumn(\"is_valid\", col(\"amount\") &gt; 0)\n        .withColumn(\"processed_at\", current_timestamp())\n        .withColumn(\"ingest_year\", year(col(\"timestamp\")))\n        .withColumn(\"ingest_month\", month(col(\"timestamp\")))\n        .withColumn(\"ingest_day\", dayofmonth(col(\"timestamp\")))\n        .withColumn(\"ingest_hour\", hour(col(\"timestamp\")))\n    )\n\n    return silver_df\n</code></pre>"},{"location":"api-reference/processors/#src.processors.silver_processor.SilverProcessor.write_to_silver","title":"<code>write_to_silver(df)</code>","text":"<p>Write processed data to silver table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Cleaned DataFrame to write</p> required Source code in <code>src\\processors\\silver_processor.py</code> <pre><code>def write_to_silver(self, df: DataFrame) -&gt; None:\n    \"\"\"\n    Write processed data to silver table.\n\n    Args:\n        df: Cleaned DataFrame to write\n    \"\"\"\n    df.write.format(\"delta\").mode(\"append\").partitionBy(\n        \"ingest_year\", \"ingest_month\", \"ingest_day\"\n    ).saveAsTable(self.db_config.silver_table)\n\n    self.logger.info(f\"Data written to {self.db_config.silver_table}\")\n</code></pre>"},{"location":"api-reference/processors/#src.processors.silver_processor.SilverProcessor.process_hour","title":"<code>process_hour(target_hour=None)</code>","text":"<p>Process a specific hour of data from bronze to silver.</p> <p>Parameters:</p> Name Type Description Default <code>target_hour</code> <code>Optional[datetime]</code> <p>Target hour to process (defaults to current hour)</p> <code>None</code> Source code in <code>src\\processors\\silver_processor.py</code> <pre><code>def process_hour(self, target_hour: Optional[datetime] = None) -&gt; None:\n    \"\"\"\n    Process a specific hour of data from bronze to silver.\n\n    Args:\n        target_hour: Target hour to process (defaults to current hour)\n    \"\"\"\n    if target_hour is None:\n        target_hour = datetime.utcnow()\n\n    start_ts = target_hour.replace(minute=0, second=0, microsecond=0)\n    end_ts = start_ts + timedelta(hours=1)\n\n    self.logger.info(f\"Processing hour: {start_ts} to {end_ts}\")\n\n    bronze_df = self.read_bronze_data(start_ts, end_ts)\n\n    if bronze_df.count() == 0:\n        self.logger.warning(\"No records found for the specified hour. Exiting.\")\n        return\n\n    silver_df = self.clean_and_transform(bronze_df)\n    self.write_to_silver(silver_df)\n</code></pre>"},{"location":"api-reference/streaming/","title":"Streaming","text":"<p>Handles streaming data from Event Hub to Delta tables.</p> Source code in <code>src\\streaming\\event_hub_streamer.py</code> <pre><code>class EventHubStreamer:\n    \"\"\"Handles streaming data from Event Hub to Delta tables.\"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        eh_config: EventHubConfig,\n        db_config: DatabaseConfig,\n        dbutils,\n        sc,\n    ):\n        \"\"\"\n        Initialize the Event Hub streamer.\n\n        Args:\n            spark: Spark session\n            eh_config: Event Hub configuration\n            db_config: Database configuration\n        \"\"\"\n        self.spark = spark\n        self.eh_config = eh_config\n        self.db_config = db_config\n        self.logger = logging.getLogger(__name__)\n        self.dbutils = dbutils\n        self.sc = sc\n\n    def _get_event_hub_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get Event Hub connection configuration.\"\"\"\n        connection_string = self.dbutils.secrets.get(\n            scope=self.eh_config.scope, key=self.eh_config.connection_string_key\n        )\n\n        return {\n            \"eventhubs.connectionString\": self.sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(\n                connection_string\n            ),\n            \"eventhubs.name\": self.eh_config.event_hub_name,\n        }\n\n    def create_streaming_dataframe(self) -&gt; DataFrame:\n        \"\"\"\n        Create a streaming DataFrame from Event Hub.\n\n        Returns:\n            Streaming DataFrame with parsed ad click events\n        \"\"\"\n        eh_conf = self._get_event_hub_config()\n\n        # Read from Event Hub\n        raw_df = self.spark.readStream.format(\"eventhubs\").options(**eh_conf).load()\n\n        # Parse JSON data\n        schema = AdClickSchemas.bronze_schema()\n        parsed_df = (\n            raw_df.selectExpr(\"CAST(body AS STRING) as json\")\n            .select(from_json(col(\"json\"), schema).alias(\"data\"))\n            .select(\"data.*\")\n        )\n\n        return parsed_df\n\n    def start_streaming(self) -&gt; StreamingQuery:\n        \"\"\"\n        Start streaming from Event Hub to bronze table.\n\n        Returns:\n            StreamingQuery object for monitoring\n        \"\"\"\n        streaming_df = self.create_streaming_dataframe()\n\n        query = (\n            streaming_df.writeStream.format(\"delta\")\n            .outputMode(\"append\")\n            .option(\"checkpointLocation\", self.eh_config.checkpoint_location)\n            .table(self.db_config.bronze_table)\n        )\n\n        self.logger.info(f\"Started streaming to {self.db_config.bronze_table}\")\n        return query\n</code></pre>"},{"location":"api-reference/streaming/#src.streaming.event_hub_streamer.EventHubStreamer.__init__","title":"<code>__init__(spark, eh_config, db_config, dbutils, sc)</code>","text":"<p>Initialize the Event Hub streamer.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session</p> required <code>eh_config</code> <code>EventHubConfig</code> <p>Event Hub configuration</p> required <code>db_config</code> <code>DatabaseConfig</code> <p>Database configuration</p> required Source code in <code>src\\streaming\\event_hub_streamer.py</code> <pre><code>def __init__(\n    self,\n    spark: SparkSession,\n    eh_config: EventHubConfig,\n    db_config: DatabaseConfig,\n    dbutils,\n    sc,\n):\n    \"\"\"\n    Initialize the Event Hub streamer.\n\n    Args:\n        spark: Spark session\n        eh_config: Event Hub configuration\n        db_config: Database configuration\n    \"\"\"\n    self.spark = spark\n    self.eh_config = eh_config\n    self.db_config = db_config\n    self.logger = logging.getLogger(__name__)\n    self.dbutils = dbutils\n    self.sc = sc\n</code></pre>"},{"location":"api-reference/streaming/#src.streaming.event_hub_streamer.EventHubStreamer.create_streaming_dataframe","title":"<code>create_streaming_dataframe()</code>","text":"<p>Create a streaming DataFrame from Event Hub.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Streaming DataFrame with parsed ad click events</p> Source code in <code>src\\streaming\\event_hub_streamer.py</code> <pre><code>def create_streaming_dataframe(self) -&gt; DataFrame:\n    \"\"\"\n    Create a streaming DataFrame from Event Hub.\n\n    Returns:\n        Streaming DataFrame with parsed ad click events\n    \"\"\"\n    eh_conf = self._get_event_hub_config()\n\n    # Read from Event Hub\n    raw_df = self.spark.readStream.format(\"eventhubs\").options(**eh_conf).load()\n\n    # Parse JSON data\n    schema = AdClickSchemas.bronze_schema()\n    parsed_df = (\n        raw_df.selectExpr(\"CAST(body AS STRING) as json\")\n        .select(from_json(col(\"json\"), schema).alias(\"data\"))\n        .select(\"data.*\")\n    )\n\n    return parsed_df\n</code></pre>"},{"location":"api-reference/streaming/#src.streaming.event_hub_streamer.EventHubStreamer.start_streaming","title":"<code>start_streaming()</code>","text":"<p>Start streaming from Event Hub to bronze table.</p> <p>Returns:</p> Type Description <code>StreamingQuery</code> <p>StreamingQuery object for monitoring</p> Source code in <code>src\\streaming\\event_hub_streamer.py</code> <pre><code>def start_streaming(self) -&gt; StreamingQuery:\n    \"\"\"\n    Start streaming from Event Hub to bronze table.\n\n    Returns:\n        StreamingQuery object for monitoring\n    \"\"\"\n    streaming_df = self.create_streaming_dataframe()\n\n    query = (\n        streaming_df.writeStream.format(\"delta\")\n        .outputMode(\"append\")\n        .option(\"checkpointLocation\", self.eh_config.checkpoint_location)\n        .table(self.db_config.bronze_table)\n    )\n\n    self.logger.info(f\"Started streaming to {self.db_config.bronze_table}\")\n    return query\n</code></pre>"}]}